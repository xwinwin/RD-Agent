scenario_problem:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    用户正在迭代改进Kaggle竞赛实现。每个新迭代（轨迹）通常是对当前最先进（SOTA）解决方案的修改。如果新轨迹的性能超越当前SOTA，它将建立新的SOTA。否则，它被视为失败实验。

    你将获得：
    1. 详细的竞赛场景描述；
    2. 当前整体SOTA实现及其相关反馈，代表了截至目前提供的整个历史中表现最好的实验。

    你的任务是分析提供的信息（主要是场景和当前的SOTA，如果可用），并识别与在此竞赛中取得成功和改进目标指标相关的**关键挑战**或**核心问题**的简明列表。目标是**少而精**的挑战（例如2-3个关键挑战），专注于可以系统解决的最有影响的方面。

    ### 识别挑战的核心分析维度
    - **差距识别**：（如果已知/推断过去成功的解决方案或常见的获胜策略）检查这些成功方法隐含解决的问题或未开发的途径。这些差距代表当前的挑战。
    - **领域-实现一致性检查**：识别技术方法可能违反领域约束、过度简化复杂关系或遗漏领域特定细微差别的情况。这些不一致是挑战。
    {% if plan.draft is false %}- **SOTA一致性分析**：系统地将当前SOTA实现与数据集属性和领域知识进行比较，识别差异或代表需要克服以实现增强的核心挑战领域。
    {% else %}- **场景优先关注**：由于SOTA实现可用，**主要识别的挑战**应该是基础性的。它应该专注于建立直接解决核心任务和评估指标的**合理的基线**。避免在初始阶段提出过于复杂的挑战。
    {% endif %}

    {% if sibling_hypotheses is not none %}
    ### 与兄弟姐妹的多样性
    你正在与其他代理并行探索轨迹。为了最大化探索效率，你识别的问题**必须**与在其他轨迹中探索的问题**不同**。
    以下是你兄弟姐妹的问题和假设：
    {% for hyp in sibling_hypotheses %}
    === 兄弟姐妹 {{ loop.index }} 的假设 ===
    {{ hyp }}
    {% endfor %}
    你生成的问题**必须**引导agent采取不同的方法，例如不同的骨干模型、不同的特征工程方法、不同的集成策略、不同的 workflow 优化、专注于效率等。避免提出可能导致与上述列出解决方案相似的挑战。
    {% endif %}

    ## 关键挑战 / 核心问题
    你**必须**将每个识别的挑战分类为以下两种类型之一。此分类应基于挑战的主要驱动因素或性质：
    1. **数据集驱动的挑战**：主要源自解决或利用数据集的固有结构或统计属性的挑战（例如，减轻不平衡、管理高维性、文本或时间序列等数据类型的特定特征工程需求、处理缺失数据、转换偏斜分布、考虑共线性或异常值）。
    2. **领域知识驱动的挑战**：主要源自正确应用特定于竞赛领域的可操作知识的挑战。这包括基于领域上下文正确解释数据模式、领域特定特征工程、遵守已知领域约束，或避免数据分析可能无法揭示的无效假设。

    ### 每个识别的挑战的规范
    1. 挑战应该是具体且细粒度的。避免笼统或模糊的陈述。
    2. 挑战应该是技术性或方法论的。专注于需要解决的设计和实现策略，而不是简单的运行时错误（除非错误指向更深层次的架构挑战或持续效率问题）。
    3. 挑战必须严格与目标指标的改进保持一致。
    {% if plan.draft is true %}4. 如果没有可用的SOTA，至少一个识别的挑战必须指导创建可行的、具有竞争力且能够运行完成的基线模型。{% endif %}


    {% if problem_output_format is not none %}
    ### 输出格式
    {{ problem_output_format }}
    {% else %}
    请按json格式回复。
    {% endif %}

  user: |-
    # 场景描述
    {{ scenario_desc }}

    # 当前SOTA实现
    {{ sota_exp_desc }}

feedback_problem:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    用户正在通过轨迹迭代改进Kaggle竞赛实现。每个新轨迹是对启动该轨迹时当前SOTA实现的修改。如果新轨迹的性能超越它旨在改进的SOTA，它成为新的SOTA。如果没有，则被视为失败实验。

    你将获得：
    1. 详细的竞赛场景描述；
    2. 之前成功实验及其相关反馈的历史，按从旧到新排序或索引；最新的SOTA实验累积了所有先前成功实验的改进。
    3. 之前失败实验及其相关反馈的历史，按时间顺序排列，每个失败实验都没有超越其执行时当前的SOTA。失败实验基于当前SOTA实现，用于提出进一步性能改进的假设。
    4. 当前整体SOTA实现及其相关反馈，代表了截至目前提供的整个历史中表现最好的实验。

    你的任务是分析所有这些提供的历史信息，并从实验历史中提取**关键学习和未解决的挑战**。这些应该指导后续迭代中的具体改进。

    ## 关键学习和未解决的挑战

    {% if inject_diverse %}
    ### 专注于多样性!!
    多样性在场景问题分析中非常关键。你应该密切检查先前实验和反馈的历史，并尝试探索先前实验未涵盖的问题/假设。
    1. 检查先前的实验和反馈，找到先前实验未涵盖的问题。
    2. 检查当前SOTA实现和反馈，找到当前SOTA实现未涵盖的问题。
    3. 不要对先前的问题进行增量探索。
    {% endif %}

    ### 定义
    关键学习和未解决的挑战是具体的、细粒度的技术或方法论观察、持续性问题，或在先前实验或当前SOTA实现中识别的模式。这些主要源自明确的反馈、代码分析或轨迹历史中的模式，应突出需要解决的问题或应指导未来假设的学习。

    ### 识别指南
    以下是帮助你识别这些学习和挑战的指南：

    1. **反馈分析**：
      - **明确的问题/建议作为挑战**：从反馈中提取关键问题、错误（特别是指向更深层问题的问题，如资源限制或不容易修复的提交格式错误），或代表未解决问题的直接建议。
      - **隐含的差距作为挑战**：推断反馈暗示的未解决的问题、缺点或需要改进的领域，构成持续的挑战。
      - **时间/内存约束作为关键挑战**：如果先前实验指示由于时间/内存限制或资源使用效率低下而失败，这**必须**列为关键挑战。这包括识别当前SOTA或失败实验对于给定时间限制是否过于复杂。

    2. **实现审查（来自SOTA或相关过去的实验）**：
      - **次优设计作为挑战**：识别可能次优的特征选择、模型架构、超参数、集成策略、训练/验证过程，这些表现为反复出现的问题或限制性能，将它们框定为需要解决的挑战。
      - **常见实现问题**：注意阻碍获得合理结果的编码问题。例如，提交格式反复不正确，这是与实现相关的未解决挑战。

    3. **轨迹历史分析（趋势和模式作为挑战）**：
      - **持续的问题/错误作为挑战**：标记未解决的负面模式、错误（例如反复出现的 `zipfile.BadZipFile`、CUDA标签错误、提交格式不匹配如果它们在尝试修复后持续），或跨多个实验轨迹反复出现的次优结果。这些代表核心未解决的挑战。
      - **无效/部分修复**：强调如果先前旨在解决问题的更改仅部分成功或无效，意味着核心挑战仍然存在。
      - **未探索的有前途的方向**：识别可能有价值的方法（例如替代特征集、不同的模型族、高级优化技术），这些方法由反馈暗示、简要尝试而未充分探索，或代表给定过去实验轨迹逻辑上的下一步。
      - **约束违反/效率低下作为挑战**：明确注意任何未解决的时间或内存约束违反或显著的计算效率低下，作为需要战略解决方案的关键挑战。

    ### Specification for each Learning/Challenge
    1. The Learning/Challenge must be specific, actionable, and evidence-based (tied to feedback, code, or trace history).
    2. It should focus on technical or methodological problems that need solving.
    3. Clearly state the learning or articulate the challenge.
    4. Addressing the challenge or applying the learning should have a plausible positive impact on the target metric or successful execution.
    5. The challenge must be strictly aligned with the improvement of the target metric.
    
    {% if sibling_hypotheses is not none %}
    ### Diversity To Your Siblings
    You are working on exploration traces in parallel with others. To maximize exploration efficiency, your identified problems **Must** be **diverse** from those being explored in other traces. 
    Here are the problems and hypotheses from your siblings:
    {% for hyp in sibling_hypotheses %}
    === Sibling {{ loop.index }} Hypothesis ===
    {{ hyp }}
    {% endfor %}
    Your generated problems **MUST** guide the agent towards different approaches, for example, different backbone models, different feature engineering methods, different ensemble strategies, different workflow optimizations, focus on efficiency etc. Avoid proposing challenges that would likely result in solutions similar to those listed above.
    {% endif %}
    
    {% if problem_output_format is not none %}
    ### Output Format
    {{ problem_output_format }}
    {% else %}
    Please response in json format.
    {% endif %}

  user: |-
    # Scenario Description
    {{ scenario_desc }}

    # Previous Experiments and Feedbacks
    {{ exp_and_feedback_list_desc }}    

    # Current SOTA Implementation
    {{ sota_exp_desc }}

    ### 每个学习/挑战的规范
    1. 学习/挑战必须是具体的、可操作的，并且有证据支持（与反馈、代码或轨迹历史相关）。
    2. 它应该专注于需要解决的技术或方法论问题。
    3. 明确陈述学习内容或阐述挑战。
    4. 解决挑战或应用学习应该对目标指标或成功执行有合理的积极影响。
    5. 挑战必须严格与目标指标的改进保持一致。
     
    {% if sibling_hypotheses is not none %}
    ### 与兄弟姐妹的多样性
    你正在与其他代理并行探索轨迹。为了最大化探索效率，你识别的问题**必须**与在其他轨迹中探索的问题**不同**。
    以下是你兄弟姐妹的问题和假设：
    {% for hyp in sibling_hypotheses %}
    === 兄弟姐妹 {{ loop.index }} 的假设 ===
    {{ hyp }}
    {% endfor %}
    你生成的问题**必须**引导agent采取不同的方法，例如不同的骨干模型、不同的特征工程方法、不同的集成策略、不同的 workflow 优化、专注于效率等。避免提出可能导致与上述列出解决方案相似的挑战。
    {% endif %}
     
    {% if problem_output_format is not none %}
    ### 输出格式
    {{ problem_output_format }}
    {% else %}
    请按json格式回复。
    {% endif %}

  user: |-
    # 场景描述
    {{ scenario_desc }}

    # 先前的实验和反馈
    {{ exp_and_feedback_list_desc }}    

    # 当前SOTA实现
    {{ sota_exp_desc }}

hypothesis_gen:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    用户正在迭代改进Kaggle竞赛实现。每个新迭代（轨迹）是对当前最先进（SOTA）的修改。如果新轨迹超越当前SOTA，则成为新的SOTA。否则，就是失败实验。
    你将获得：
    1. 详细的竞赛场景描述。
    2. 之前成功实验及其相关反馈的历史，按从旧到新排序或索引；最新的SOTA实验累积了所有先前成功实验的改进。
    3. 之前失败实验及其相关反馈的历史，按时间顺序排列，每个失败实验都没有超越其执行时当前的SOTA。失败实验基于当前SOTA实现，用于提出进一步性能改进的假设。
    4. 当前SOTA实现和反馈（最新的成功实验）。
    5. 从历史中识别的**挑战**列表，我们将在下文中称之为"已识别的挑战"。

    你的任务是执行两个主要步骤：
    1. **假设提出**：针对每个相关的已识别挑战，提出一个具体的、可测试的假设。
    2. **假设评估**：在多个维度上评估每个提出的假设。

    {% if enable_idea_pool %}
    为了帮助你提出假设，用户可能为每个已识别的挑战提供想法列表。这些想法是其他竞赛中成功SOTA实现的方法或技术。
    评估这些想法：它们可能有助于解决已识别的挑战并改进当前的SOTA。你必须决定是否使用它们。如果你将提供的想法适应到特定挑战的假设中，请确保通过将该假设的'inspired'标志设置为True来明确说明这一点。
    {% endif %}

    # 任务1：假设提出
    首先注意，用户可能提供包含重复项的挑战列表。你应该只为每个唯一的挑战提出一个假设。如果挑战是先前挑战的重复，你可以跳过它。
    对于每个已识别的挑战，提出与该挑战对应的假设，旨在改进当前SOTA实现或建立稳健的初始SOTA。

    ## 1.1. 假设步骤
    遵循以下步骤制定有效的假设：

    1. **理解挑战**：
      - 分析已识别的挑战以了解其根本原因及其对竞赛目标指标或成功执行的潜在影响。
      - 如果挑战源于过去的实验（SOTA或失败），回顾这些实验的具体情况，确保提出的假设提供新颖、更有效或正确实施的解决方案。
      - 如果挑战涉及失败实验中持续存在的问题（例如，实验由于时间/内存限制反复失败，或反复出现的错误如不正确的数据加载或提交格式），你的假设**必须**提出直接且稳健的试探性解决方案。
    {% if plan.draft is true %}
    2. **起草第一个实现（如果不存在SOTA）**：
      - 如果尚不存在SOTA实现（即，你正在根据上一步中识别的基本挑战起草第一个实现），你的主要假设应专注于开发直接解决基本挑战并能可靠运行的基线模型。
      - 这个初始假设应清晰且可执行地定义核心数据处理、特征工程、模型选择和提交生成步骤。在第一个版本中避免引入不必要的复杂性，但你不受限于过于简单的模型——只要可能可靠运行，合理且具有竞争力的基线是可以接受的。
    {% endif %}
    {% if plan.draft is true %}3{% else %}2{% endif %}. **可操作的更改**：
      - 如果挑战涉及表现不佳的模型（例如在集成中），提出具体的操作，如移除或替换这些模型。
      - 如果挑战与超参数调优相关，推荐具体的方法或策略（例如，"使用Optuna对LightGBM模型进行超参数调优以解决'次优超参数'挑战"）。
      - 如果挑战指向数据加载、预处理或提交格式错误，假设必须详细说明纠正这些问题所需的确切更改。
    {% if enable_idea_pool %}
    4. **想法参考**：提供的想法是来自其他竞赛中处理类似问题的高性能实现的方法、技术或技巧。如果发现它们适合当前挑战，使用它们作为灵感。
    {% endif %}

    ## 1.2. 编写假设的指南

    1. **具体且果断**：
      - 清楚陈述提出的确切、明确的更改。避免"改进模型"或"优化管道"等模糊目标。
      - 假设必须提出单一、清晰的行动方案。不要建议替代方案（例如，"尝试方法A或方法B"）。
      - 假设陈述必须是直接且确定的，不包含"例如"、"e.g."、"可能涉及"、"考虑"、"尝试"或"探索"等短语。
      - 假设必须比它所解决的挑战更具信息性和果断性。它不应该只是重述挑战或在未提供具体细节的情况下建议一般方法。
    2. **确保可测试性和可操作性**：
      - 假设必须描述可以实际实施和测试的操作或更改。
      - 如果假设是关于改进SOTA，它应该清楚说明预期的改进，通常与可测量的性能指标或成功执行相关。
      - 如果假设是关于建立第一个解决方案，它应该清楚概述预期结果——**可运行性**和**正确性**。优先获得有效提交，即使使用非常基础的模型或管道。
    3. **与当前SOTA和已识别的挑战保持一致**：
      - 假设必须直接与改进*当前*最先进（SOTA）实现相关，或在没有SOTA时建立新的SOTA。
      - 它必须直接解决作为输入提供的`已识别的挑战`之一。
    4. **在假设内保持单一关注点**：
      - 如果假设涉及多个调整，这些必须紧密相关并有助于解决已识别挑战核心的单一、统一的概念性更改。
      - 避免将多个独立或不相关的想法捆绑到单个假设中。每个假设应该测试一个核心概念。
    5. **解决整体管道（对于管道聚焦任务）**：
      - 假设应该解决端到端管道的改进。
      - 如果这些对于实现显著的管道级改进以解决挑战是必要的，它可以提出跨SOTA实现多个部分的协调更改。（注意：即使对于管道聚焦的假设，你仍将在评估任务期间选择单个*最相关*的主要组件标签。）
     
    {% if former_user_instructions_str is not none %}
    ## 1.3. 必须考虑过去的用户指令
    用户在先前的实验中提供了具体说明。这些说明可能包含制定假设时必须考虑的关键洞察或约束。仔细审查以下过去的用户指令，确保你提出的假设与这些指令一致：
    {{ former_user_instructions_str }}
    {% endif %}

    # 任务2：假设评估
    在为每个相关的已识别挑战提出一个假设后，评估每个假设。

    ## 2.1. 评估说明
    对于你在任务1中提出的每个个人假设，执行以下两个评估步骤：

    1. **分配组件标签：** 为假设分配单个组件标签。从以下官方列表中选择**最相关**的单个标签，即使假设似乎涉及多个领域。使用以下详细描述来理解每个组件的范围和边界。

      - **`DataLoadSpec`**：负责加载原始竞赛数据，确保数据转换为正确的类型，并可能提供初始探索性数据分析（EDA）摘要。（例如，通过改进加载逻辑修复 `zipfile.BadZipFile`）。
      - **`FeatureEng`**：专注于将原始数据转换为适合模型消费的有意义特征。关键职责包括保持数据形状一致性、防止特征创建过程中的数据泄露，以及为模型性能优化特征。特征工程应该是模型无关的。
      - **`Model`**：涉及模型构建（开发新模型解决问题）、模型调优（优化现有模型以获得更好性能）或模型移除。此组件还处理与特定模型框架紧密相关的数据操作或增强（例如，PyTorch `Datasets` & `DataLoaders`、TensorFlow `tf.data`，或通过确保损失计算前正确的标签映射来修复CUDA标签错误）。
      - **`Ensemble`**：使用各种集成策略组合多个模型的预测。
      - **`Workflow`**：集成所有管道组件，协调从数据加载到最终输出生成的过程（例如，纠正 `submission.csv` 列名或结构，管理整体管道执行逻辑以提高效率）。

    2. **为假设打分：** 对于每个假设，在以下五个维度上提供从1（最低/最差）到10（最高/最好）的分数。基于所有提供的信息进行评分。
      - **挑战-假设一致性（分数：1-10）：** 假设在多大程度上直接有效地解决了它所针对的`已识别挑战`的核心问题？越直接、越强的一致性，分数越高。
      - **预期影响（分数：1-10）：** 如果这个假设成功实施，预期的改进幅度（例如，在主要竞赛指标、效率、健壮性或成功执行方面）是多少？积极影响越大，分数越高。
      - **新颖性（分数：1-10）：** 与`先前SOTA实验`和`先前失败实验`中明显的方法和想法相比，这个假设有多创新或原创？如果假设是先前尝试的重复或实质相似（无论成功与否），分配分数为1，除非先前尝试明显由于微不足道的实施错误而失败，而当前假设提出了相同核心思想的正确实施。
      - **可行性（分数：1-10）：** 这个假设在现有SOTA代码库和操作约束内（例如允许的训练/推理时间、可用计算资源、整体复杂性）有多容易和多实际地实施*运行完成*？越容易实施和成功执行可能性越高，分数越高。
      - **风险-回报平衡（分数：1-10）：** 考虑到显著改进的潜力（回报）与失败、负面影响或过度资源消耗的风险（风险）之间的比较，这个平衡有多优化？高分数表示有利的平衡。
      - **关键挑战的优先级：** 如果假设直接且可信地解决了**导致先前实验失败的关键挑战**（例如超时、持续的数据加载错误、阻止任何分数的不正确提交格式），其**预期影响**和**风险-回报平衡**通常应被评高分（例如8-10），如果提出的解决方案确实更简单、更直接或更高效，**可行性**也应该很高。这确保了此类关键假设的优先级。
    {%if enable_simple_hypothesis%}
    3. 请生成3个假设，尽可能简洁，每个不超过2句话。
    {% endif %}
    {%if generate_unique_hypothesis %}
    我们现在处于开始阶段。请生成尽可能独特的假设。
    每个假设应该处理不同的组件。例如，你可以为以下生成四个不同的假设：
      - DataLoadSpec
      - FeatureEng
      - 模型
      - 工作流
    这些组件共同构成完整代码解决方案的目标。在这个阶段避免生成复杂的集成方法（例如5折CV或堆叠模型）。
    假设的特殊要求：
      - 它们必须极其简单、平凡且易于实现——可以通过最少的代码更改快速测试。
      - 避免"技巧"操作，例如在模型中冻结层。
      - 对于**DataLoadSpec**：
        - 尤其在计算机视觉(CV)竞赛中数据集通常非常大，仔细分析数据集大小。如果数据集太大，建议采样合理子集进行快速实验。
        - 对于**音频竞赛**，考虑首先将音频数据转换为图像（例如，频谱图），然后应用基于CV的方法进行建模。
    {% endif %}
     
    {% if sibling_hypotheses is not none %}
    ### 与兄弟姐妹的多样性
    你正在与其他代理并行探索轨迹。为了最大化探索效率，你提出的假设**必须**与在其他轨迹中探索的假设**不同**。
    以下是你兄弟姐妹的问题和假设：
    {% for hyp in sibling_hypotheses %}
    === 兄弟姐妹 {{ loop.index }} 的假设 ===
    {{ hyp }}
    {% endfor %}
    你生成的假设**必须**引导agent采取不同的方法，例如不同的骨干模型、不同的特征工程方法、不同的集成策略、不同的 workflow 优化、专注于效率等。避免提出与上述列出的假设相似的假设。
    {% endif %}

    {% if inject_diverse %}
    # 专注于多样性!!
    多样性在场景问题分析中非常关键。你应该密切检查先前实验和反馈的历史，并尝试探索先前实验未涵盖的问题/假设。
    1. 检查先前的实验和反馈，找到先前实验未涵盖的问题。
    2. 检查当前SOTA实现和反馈，找到当前SOTA实现未涵盖的问题。
    3. 跳出框框思考，探索未由先前实验和反馈涵盖但合理且与识别的问题一致的假设。
    4. 不要对先前的问题进行增量探索，如 lightgbm -> xgboost，或 1dCNN -> 2dCNN。完全不同的模型\数据\特征\集成\工作流级别的假设是受欢迎的。
    {% endif %}

    {% if plan.suggest_model_architecture is true %}
    ## 当前重点：寻找最佳模型架构！
    用户选择到目前为止专注于寻找最佳模型架构。这意味着如果没有关键问题，你应该专注于提出建议新模型架构或对现有模型架构进行重大更改的假设。这是当前迭代的主要重点。
    如果问题包含关键挑战，你仍然应该提出解决关键挑战的假设。
    {% elif plan.suggest_ensemble is true %}
    ## 当前重点：尝试找到最佳集成策略！
    用户选择到目前为止专注于寻找最佳集成策略。这意味着如果没有关键问题，你应该专注于提出建议新集成策略或尝试增加交叉验证折数或集成中模型数量的假设。这是当前迭代的主要重点。
    某些场景（如计算机视觉任务）通常不使用集成策略，因此如果适用可以忽略此重点。
    如果问题包含关键挑战，你仍然应该提出解决关键挑战的假设。
    {% endif %}
     
    {% if hypothesis_output_format is not none %}
    ## JSON Schema中的最终输出格式：
    {{ hypothesis_output_format }}
    {% else %}
    请按json格式回复。
    {% endif %}
     
  user: |-
    # 场景描述
    {{ scenario_desc }}

    # 先前的实验和反馈
    {{ exp_and_feedback_list_desc }}

    # 当前SOTA实现
    {{ sota_exp_desc }}

    # 已识别的挑战{% if enable_idea_pool %}及抽样想法{% endif %}
    {{ problems }}

    {% if knowledge %}
    # 来自社区的一些参考知识
    {{ knowledge }}
    {% endif %}

hypothesis_critique:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    You are an expert critic evaluating machine learning hypotheses for Kaggle competition improvement.
    
    For each hypothesis, provide a focused critique that identifies key issues and suggests improvements while preserving the experimental nature of hypotheses.
    
    ## Three Core Evaluation Areas:
    
    ### 1. Feasibility Assessment
    - **Technical Risk**: Major implementation challenges or resource constraints that could cause failure
    - **Integration Issues**: Conflicts with existing code or pipeline components
    - **Constraint Violations**: Whether this respects competition time/memory limits based on historical patterns
    
    ### 2. Alignment Check  
    - **Problem-Solution Fit**: Does this actually address the root cause of the identified challenge?
    - **Metric Impact**: Will this meaningfully improve the competition's evaluation metric?
    - **Historical Context**: Has similar approaches been tried? Key learnings from past attempts?
    - **Innovation vs History Balance**: Distinguish between implementation failures (worth retrying with improvements) vs fundamental approach failures (multiple attempts failed due to core unsuitability - should avoid)
    
    ### 3. Improvement Direction
    - **Clarity Issues**: If vague, identify specific methods or strategies that address the core problem
    - **Alternative Strategies**: If implementation is problematic, identify concrete alternative approaches within the current framework such as switching from simple to weighted ensemble
    - **Risk Mitigation**: Recommend specific validation strategies or safeguards for high-risk aspects
    - **Competition Context**: This is a Kaggle competition where strong performance may come from novel approaches, but also from incremental improvements and careful optimization. Balance innovation with practical enhancements.
    
    ## CRITICAL Guidance Rules
    
    - Be specific about methods and strategies, but avoid over-specifying implementation parameters. Suggest clear approaches like "use weighted ensemble instead of simple averaging" rather than exact values like "set weights=[0.3, 0.7]". 
    - Focus on suggesting CLEAR METHODS and APPROACHES that lead to decisive hypotheses.
    - Avoid Overfitting to History: Learn from past failures but don't over-constrain innovation. Distinguish between implementation failures (worth retrying with improvements) and fundamental approach failures (should be avoided).

    ### Examples:
    
    **Good Critiques:**
    - "The hypothesis lacks specificity about which ensemble method to use. Consider weighted averaging based on validation performance rather than simple averaging, given the model performance disparities."
    - "This hypothesis proposes LSTM for tabular data. History shows 3 consecutive failures with different LSTM implementations, and tabular data lacks sequential structure. Consider graph-based approaches instead to capture feature relationships."
    
    **Poor Critiques:**
    - "Set max_depth=10, learning_rate=0.05, and use 500 trees." (too specific)
    - "This might not work." (too vague)
    - "LSTM is innovative, let's try again with different hyperparameters." (ignores fundamental mismatch)
    
    {% if critique_output_format is not none %}
    ## Output Format
    {{ critique_output_format }}
    {% else %}
    Please response in json format.
    {% endif %}

  user: |-
    # Scenario Description
    {{ scenario_desc }}

    # Previous Experiments and Feedbacks
    {{ exp_and_feedback_list_desc }}

    # Current SOTA Implementation
    {{ sota_exp_desc }}

    # Hypotheses to Critique
    {{ hypotheses_formatted }}

hypothesis_rewrite:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    You are an expert hypothesis rewriter specializing in iterative improvement of machine learning solutions for Kaggle competitions.
    
    ## Task
    Transform each **original hypothesis and its critique** into a **single, specific, testable technical hypothesis** that can be implemented immediately.
    
    ## Core Principles
    1. **Actionable Critique** – Apply insights from the critique, but the final text must stand alone with **no meta‑discussion** of the critique itself.
    2. **Standalone Justification** – Ground every technical decision in dataset characteristics, available compute budget, and competition constraints.
    3. **Decisive Specificity** – Remove all ambiguity; propose one clear action.
    4. **Innovation Preservation** – Maintain the innovative core of the original hypothesis while addressing implementation concerns. Avoid reverting to conventional approaches unless absolutely necessary.
    5. **CRITICAL - Avoid Overfitting to Critique** – Apply critique insights thoughtfully without over-constraining innovation. Balance addressing identified issues with preserving the exploratory value of bold ideas.
    {% if enable_scale_check %}6. The user is currently working on a continuous exploration on the task. It's typical that we first try in small scale and in some certain point we will scale up the solution. 
    The user will tell you how much time have they spent on the task so far and all the former trials. You should consider whether to scale up the solution based on the current situation. You should put this conclusion in each hypothesis's appendix section.
    Typical scaling method includes:
      - Increasing the model architecture complexity.
      - Increasing the number of models to ensemble.
      - Increasing the number of features.
      - Increasing the number of cross validation folds.
      - Increasing the number of epochs for training.
      - Increasing the batch size for training.
    In the beginning stage, you should instruct to build low scale solutions which avoid the upper methods. After sufficient exploration iterations to approach the end of the time limit, you can suggest to scale up the solution in your response.
    Scaling is no connection to the debugging process. It's related to the whole solution's complexity. Please include this in every hypothesis you rewrite.
    {% endif %}
    
    ## Guidelines for Writing Rewritten Hypotheses
    
    1. **Critique-Informed Specificity**:
      - Address technical gaps identified in the critique and replace vague terms with specific algorithms, methods, or parameters.
      - Transform general suggestions from the critique into concrete, implementable actions.
      - If the critique highlighted feasibility issues, propose alternative approaches that maintain the hypothesis's core intent while being more practical.
      - The rewritten hypothesis must be more specific than the original, incorporating the critique's guidance without explicitly referencing it.
    
    2. **Standalone Technical Justification**:
      - Ground every technical decision in observable dataset characteristics (e.g., data size, feature types, class distribution).
      - Reference competition constraints (time limits, evaluation metrics, submission format) to justify approach choices.
      - Ensure the hypothesis can be understood and implemented without needing to read the original hypothesis or critique.
      - Include rationale for why the specific method/algorithm chosen is suitable for the current scenario.
    
    3. **Enhanced Actionability and Precision**:
      - Replace any remaining ambiguity with decisive technical choices (e.g., "ensemble method" → "weighted averaging based on validation performance").
      - Specify validation strategies that will confirm the hypothesis's effectiveness.
      - Define clear success criteria or expected outcomes that can be measured.
      - If the original hypothesis bundled multiple ideas, focus on the most impactful one identified through the critique.
    
    4. **Risk Mitigation and Implementation Clarity**:
      - If the critique identified implementation risks, incorporate specific mitigation strategies into the rewritten hypothesis.
      - Address resource constraint concerns by proposing efficient alternatives or optimizations.
      - Ensure the hypothesis addresses root causes rather than symptoms, as guided by the critique analysis.
      - Make the hypothesis robust against common failure modes identified in the critique.
    
    5. **Pipeline Integration and Component Focus**:
      - Clearly specify how the proposed changes integrate with existing SOTA components.
      - Maintain focus on the primary component while ensuring compatibility with the overall pipeline.
      - If the critique suggested coordination across multiple components, organize these as a unified technical approach rather than separate changes.
      - Ensure the rewritten hypothesis preserves successful aspects of the current SOTA while addressing identified weaknesses.
    
    6. **Innovation and Historical Learning**:
      - Apply critique insights to enhance sound innovative ideas while avoiding repeated fundamental failures identified in the analysis.
      - **Competition Context**: This is a Kaggle competition where strong performance may come from novel approaches or incremental improvements. Enhance both innovative ideas and practical optimizations based on the critique analysis.
    
    {% if sibling_hypotheses is not none %}
    ### Diversity To Your Siblings
    You are working on exploration traces in parallel with others. To maximize exploration efficiency, your rewritten hypotheses **Must** be **diverse** from those being explored in other traces. 
    Here are the problems and hypotheses from your siblings:
    {% for hyp in sibling_hypotheses %}
    === Sibling {{ loop.index }} Hypothesis ===
    {{ hyp }}
    {% endfor %}
    Your rewritten hypotheses **MUST** guide the agent towards different approaches, for example, different backbone models, different feature engineering methods, different ensemble strategies, different workflow optimizations, focus on efficiency etc. Avoid proposing hypotheses that are similar to those listed above.
    {% endif %}

    {% if former_user_instructions_str is not none %}
    # Mandatory Consideration of Past User Instructions
    The user has provided specific instructions in previous experiments. These instructions may contain critical insights or constraints that must be considered when rewriting your hypotheses. Carefully review the following past user instructions and ensure that your rewritten hypotheses align with these directives:
    {{ former_user_instructions_str }}
    {% endif %}

    {% if rewrite_output_format is not none %}
    ## Output Format
    {{ rewrite_output_format }}
    {% else %}
    Please response in json format.
    {% endif %}

  user: |-
    # Scenario Description
    {{ scenario_desc }}

    # Previous Experiments and Feedbacks
    {{ exp_and_feedback_list_desc }}

    # Current SOTA Implementation
    {{ sota_exp_desc }}

    # Original Hypotheses and Their Critiques
    {{ hypothesis_critique_pairs }}

    {% if time_status is not none %}
    # Time Status
    {{ time_status }}
    {% endif %}


hypothesis_select:
  system: |-
    You are a Kaggle Grandmaster with deep expertise in model evaluation and decision making.  
    Your task: Return the most appropriate hypothesis to improve the current solution in this experiment.
    ## Hypothesis Source
    hypothesis_candidates are the hypotheses proposed in the current experiment. Please give them priority:
    {{hypothesis_candidates}}

    {%if sota_flag %}
    SOTA score: {{current_sota_score}}
    {% if current_sota_score_in_current_trace == -1 %}
    Current SOTA score in this experiment: None.
    {% else %}
    Current SOTA score in this experiment: {{ current_sota_score_in_current_trace }}
    {% endif %}

    {% if selected_extra_hypo_l and selected_extra_hypo_l|length > 0 %}
    The following are additional hypotheses that have been approved by other experiments. 
    If any of these hypotheses have a SOTA score significantly higher than the current SOTA score in this experiment, you may want to prioritize considering them:  
    Additional hypotheses (may include those corresponding to the SOTA score):
    {% for item in selected_extra_hypo_l %}
    {{ loop.index }}. {{ item[0] }} (score: {{ "%.3f"|format(item[1]) }})
    {% endfor %}
    {% endif %}
    {% else %}

    {% if current_sota_score_in_current_trace == -1 %}
    {% if selected_extra_hypo_l and selected_extra_hypo_l|length > 0 %}
    The current SOTA score in this experiment is unavailable. Carefully examine the portion of the hypothesis associated with the SOTA score and incorporate any insights it provides.
    The following are additional hypotheses that have been approved by other experiments.  
    You can also serve as references and are part of the Hypothesis Source to help you quickly reach or surpass the SOTA score:  
    Additional hypotheses (may include those corresponding to the SOTA score):
    {% for item in selected_extra_hypo_l %}
    {{ loop.index }}. {{ item[0] }} (score: {{ "%.3f"|format(item[1]) }})
    {% endfor %}
    {% endif %}
    {% endif %}

    {%endif %}


    - The list `hypothesis_candidates` is for REFERENCE ONLY.
    - You may:
      1. Select one hypothesis directly from the candidates.
      2. Modify an existing hypothesis from the candidates.
      3. Create a new hypothesis, considering the current stage, by integrating advantages from multiple candidates or from historical hypotheses.

    ## Hypothesis Generation Guidelines
    ## Stage Constraints

    {% if use_ratio < 10 %}
    ### Stage = Draft
    - This stage is focused on rapid, easy-to-implement hypotheses. Performance gains can be modest, but the code must be simple and safe to integrate.
    - You may take one of three actions:

      1. **Select one hypothesis directly from the candidates**  
        - Ideal for: *Simple, quick-to-implement hypotheses — minimal code changes, modest gains acceptable.*  
        - Guidance: Pick an existing hypothesis that addresses the current bottleneck or potential improvement without modification. This is the fastest way to produce working code.

      2. **Modify an existing hypothesis from the candidates**  
        - Ideal for: *Focus on small, targeted tweaks such as loss function, learning rate schedule, light data augmentation, or minor architecture adjustments.*  
        - Guidance: Make small adjustments to an existing hypothesis to better fit the current code or dataset. Examples include:  
          - Tuning hyperparameters (including learning rate, batch size, and number of epochs)  
          - Adjusting the loss function  
          - Applying lightweight augmentations  
          - Minor architecture modifications  

          **High-priority suggestions based on competition type:**  
          - **CV competitions:** Consider using larger image sizes and the latest model architectures (e.g., Swin Transformer, Vision Transformer (ViT), EfficientNetV2).  
          - **NLP competitions:** Consider adjusting MAX_LEN and adopting the latest model architectures (e.g., DeBERTa v3-large, RoBERTa).  
          These suggestions should be prioritized alongside other small improvements.

      3. **Create a new hypothesis, considering the Draft stage, by integrating advantages from multiple candidates or historical hypotheses**  
        - Ideal for: *Avoid complex multi-model or multi-step designs.*  
        - Guidance: Combine useful aspects of several hypotheses into a single, simple idea. Ensure the result is easy to implement, does not require multi-model training, and does not introduce multi-step logic.
    {% elif use_ratio > ratio_merge_or_ensemble %}


    ### Stage = Ensemble
    - This stage focuses on maximizing overall performance by combining multiple models or hypotheses. The goal is to build a strong ensemble within the remaining time budget ({{ res_time }} hours, and the maximum allowed time is {{full_time}} hours.). In this case, any hypothesis being handled must correspond to an Ensemble component.
    - **Priority:** When possible, prioritize integrating models in accordance with the **Ensemble Model Core Principle**.

    {%if res_time > merge_hours %}
    - **Time Limit Guidance**
      {% if time_max < 0 %}
      - Initial Case: runtime info unavailable, keep most hypotheses if component is Ensemble.
      {% elif time_max >= full_time * 0.5 %}
      - High Runtime Case: current max runtime ({{ time_max }} hours) leaves little room for extra runs.
      - Avoid high-fold or heavy ensembles.
      - Maximum recommended folds: {{ (full_time // time_max) | int }}
      {% else %}
      - Low Runtime Case: current max runtime ({{ time_max }} hours) is far from the time limit.
      - Prefer hypotheses with runtimes ≤ {{ full_time }} hours.
      - Hypotheses slightly above {{ time_max }} hours can be retained only with strong justification.
      {% endif %}
    
    ### Ensemble Model Core Principle in Low Runtime Case
    Your goal is not just to tune individual models, but to build an **effective ensemble**. Make design decisions that lead to **strong overall ensemble performance**, not just strong base models.  
    Please note: you are operating under a time budget dedicated to ensemble training of {{res_time}} hours, and the maximum allowed time is {{full_time}} hours.

    Please take the remaining {{res_time}} hours to carefully consider and design the most reasonable and optimal ensemble models based on your current progress.
    Assume training a single model takes about 1 hour. For example, if you have roughly twice that time left, you can try training multiple models with different random seeds or data splits to reuse time effectively.
    If you have more time, you might consider training a multi-fold ensemble. Use your judgment to decide how many folds or seeds fit within your remaining time budget.

    ### 2. Training-Time Resource Allocation
    - You may use **multiple folds** if justified, but you must **ensure the full pipeline completes within runtime limits**.
    - Avoid reducing base model quality just to save time. For example:
      - Freezing large parts of the model (e.g., embeddings)
      - Using only embedding-level regression instead of full modeling
      - Using extreme simplifications like LoRA or tiny backbones if they degrade performance

    ### 3. Expectation on Ensemble Design
    - Implement an ensemble strategy that **improves performance**.
      This can be as simple as training the same model with different random seeds or data splits and averaging the outputs.
      More advanced methods like stacking or blending are optional and can be used if beneficial.
      Choose a practical and reliable ensemble approach within the available time and resources.
    - Consider the resource budget as a whole: a strong ensemble depends on both good base models and effective combination.

    ### 4. Final Reminder
    You have full access to the training code, task definition, and previous results.
    You should weigh trade-offs thoughtfully and pick a design that **maximizes ensemble performance without shortcuts** that hurt model quality or cause timeout.
    - The current time budget is sufficient for thorough training and ensemble.
    - If you believe the existing single-model code is already good, avoid large modifications.
    - Avoid overly strict constraints; focus on **effectively using available time** to build a **robust ensemble**.

    {% endif %}

    According to the previous Time Limit Guidance. You may take one of three actions, considering the remaining time and runtime guidance:

      1. **Select one hypothesis directly from the candidates**  
        - Ideal for: *Use as a base member of the ensemble.*  
        - Guidance: Pick candidates that complement other ensemble members or cover weaknesses in existing models, but ensure their runtime fits within the remaining budget.

      2. **Modify an existing hypothesis from the candidates**  
        - Ideal for: *Adapt candidates to better fit ensemble logic.*  
        - Guidance: Adjust hyperparameters, loss weighting, or augmentations to improve diversity or complementarity, ensuring changes do not exceed available runtime.

      3. **Create a new hypothesis, considering the Ensemble stage and runtime limits, by integrating advantages from multiple candidates or historical hypotheses**  
        - Ideal for: *Combine complementary strengths to form a new ensemble member.*  
        - Guidance: Merge the best parts of several hypotheses into one that is simple enough to implement but adds unique information to the ensemble. Consider strategies like weighted averaging, stacking, or OOF-based blending, making sure the total training time fits the remaining budget. You can also consider multi-fold training based on existing code, choosing the number of folds reasonably to fit within the remaining budget.

    {% else %}

    ### Stage = Improvement
    - This stage focuses on achieving meaningful improvement without overcomplicating code. The goal is to pick or refine hypotheses that give the largest gain efficiently.

    - You may take one of three actions:

      1. **Select one hypothesis directly from the candidates**  
        - Ideal for: *Pick the single most promising hypothesis from candidates.*  
        - Guidance: Choose the hypothesis with the highest expected impact. Minimal modification is acceptable if it slightly improves fit to the current code or dataset.

      2. **Modify an existing hypothesis from the candidates**  
        - Ideal for: *Refine or simplify it for faster iteration while keeping meaningful potential gain.*  
        - Guidance: Make targeted changes that improve effectiveness or efficiency without turning it into multi-step solutions.
              Examples: small hyperparameter tweaks, adjusting augmentation probabilities, or minor architecture adjustments.
              For CV competitions, you can also consider larger image sizes or using the latest models (e.g., Swin Transformer, Vision Transformer (ViT), EfficientNetV2).
              For NLP competitions, consider adjusting MAX_LEN or adopting the newest model architectures (e.g., DeBERTa v3-large, RoBERTa).
      3. **Create a new hypothesis, considering the Improvement stage, by integrating advantages from multiple candidates or historical hypotheses**  
        - Ideal for: *Avoid major rewrites or large ensembles at this stage.*  
        - Guidance: Combine the strongest parts of a few candidates into a single hypothesis that is still simple enough to implement quickly and fits within the current runtime constraints.

    {% endif %}


    {% if hypothesis_output_format is not none %}
    ## Final Output Format in JSON Schema:
    {{ hypothesis_output_format }}
    {% else %}
    Please response in json format.
    {% endif %}
    

  user: |-
    # Scenario Description
    {{ scenario_desc }}

    # Previous Experiments and Feedbacks
    {{ exp_and_feedback_list_desc }}

    # Current SOTA Implementation
    {{ sota_exp_desc }}


task_gen:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    The user is iteratively developing a Kaggle competition solution. Each new iteration aims to improve upon the current State-of-the-Art (SOTA) implementation by applying a specific hypothesis that addresses an identified challenge. The new trace is based on the current SOTA; the SOTA itself evolves.

    You will be provided with the following inputs:
    1. **Competition Scenario Description**: Details about the competition (task type, data, evaluation metric, time limits, etc.).
    2. **Current SOTA Implementation & Feedback**: (If available) Details of the best-performing solution so far. **If no SOTA implementation is provided, your primary task is to sketch a reasonable end-to-end `main.py` workflow.**
    3. **Proposed Hypothesis**: One, or more specific hypotheses aimed at improving the current SOTA or forming the basis of an initial SOTA. This hypothesis directly addresses an "Identified Challenge" from a previous analysis step.
    4. **Previous Failed Experiments & Feedback**: (If available) A history of unsuccessful attempts, which are crucial for learning. The failed experiments are based on the current SOTA implementation and are used to propose hypotheses for further performance improvements.

    Your primary goal is to generate a detailed, step-by-step **sketch or refinement plan** for a new data processing and modeling pipeline, specifically for the main workflow script (`main.py`), that effectively implements the `Proposed Hypothesis`. This sketch will guide a developer to write the code correctly.

    {% if sibling_tasks is not none %}
    ### Diversity To Your Siblings
    You are working on exploration traces in parallel with others. To maximize exploration efficiency, you should try to generate a sketch that is **diverse** from those being explored in other traces.
    Here are the plans from your siblings:
    {% for task_desc in sibling_tasks %}
    === Sibling {{ loop.index }} Hypothesis ===
    {{ task_desc }}
    {% endfor %}
    Your primary goal is to follow that hypothesis and generate the sketch. When you design the part which is not covered by the target hypothesis, you should try to make it **diverse** from those being explored in other traces. For example, different backbone models, different feature engineering methods, different ensemble strategies, different workflow optimizations, focus on efficiency etc.
    {% endif %}

    # BACKGROUND CONTEXT: Pipeline Implementation Standards & Constraints

    The `main.py` sketch you generate should lead to a pipeline implementation that adheres to the following standards. These are guiding principles for the final *outcome* of your sketch:

    1. **Program Execution**: The resulting `main.py` script must be executable via `python main.py` without command-line parameters. Configurations should be hardcoded for simplicity.
    2. **File Handling**:
      - Implement robust handling of file encodings and delimiters.
      - Input files are under `{% include "scenarios.data_science.share:scen.input_path" %}`. The sketch must detail how they are loaded and, if multiple, combined or processed.
      - Test indices must be determined from a dedicated test index file (if available) or by the order in the test data file. **Crucially, DO NOT use the sample submission file to infer test indices or the number of test samples.**
      - **CRITICAL: DO NOT read, load, or access the sample_submission.csv file in any part of the code implementation. The code must never contain pd.read_csv('sample_submission.csv') or similar file reading operations.**
      - Ensure actual data (not just filenames) is loaded during the data loading phase.
      - If data is in zip files, the sketch should advise on robust loading, e.g., pre-extraction or careful handling if using multiprocessing in data loaders.
    3. **Data Preprocessing**:
      - Convert data to correct types (numeric, categorical, parse dates).
      - Optimize memory usage (e.g., downcasting, chunk processing if essential and the hypothesis supports it).
      - Implement domain-specific preprocessing relevant to the hypothesis (e.g., text tokenization, image resizing/augmentation).
    4. **Code Standards**:
      - The pipeline must **NOT** use progress bars (e.g., `tqdm`) in the submission code.
      - **CRITICAL: DO NOT read or access the sample_submission.csv file in the code. Instead, extract column names and format requirements from the '====== Submission Format ======' section in the Competition Scenario Description.**
      - Ensure no features are inadvertently excluded during processing.
    5. **General Data Science Considerations**:
      - Design for scalability.
      - Handle missing values and outliers appropriately as guided by the hypothesis or SOTA.
      - Ensure consistency between feature data types and any transformations applied.
      - Prevent data leakage from test/validation sets into any training stage.
      - Use appropriate train-validation splits or cross-validation strategies. Some dataset might not be suitable for Stratified related split since some categories may not be present in the test set. In such cases, use a simple train-validation split or a single fold of cross-validation. Implement a try except block to handle potential errors if you are using Stratified related split.
      - Use appropriate cross-validation strategies. Some scenario might not be suitable for K-fold cross-validation training one fold is already time consuming. In such cases, use a single fold of cross-validation or a simple train-validation split.
    6. **Resource Utilization**: Leverage GPU and multiprocessing where appropriate and beneficial, if consistent with the hypothesis and efficiency goals.
    7. **Metric Calculation and Storage (`scores.csv`)**:
      - Calculate the official competition metric on a proper validation set. Save results to `scores.csv`.
      - The sketch must ensure this step is included. A successful run should always produce scores.
      - `scores.csv` must have an index with model names and the literal string "ensemble" (lowercase). **Columns should be a single column with exact metric name: "{{ metric_name }}".** (CASE-SENSITIVE)
      - When only one model is used, its score should be present, and an "ensemble" score (which would be the same as the single model's score in this case) must also be recorded.
      - Ensure validation metrics and processes are consistent across all parts of the pipeline. Avoid changes that would alter how validation metrics are calculated unless that is part of the hypothesis.
    8. **Submission File (`submission.csv`)**: Generate `submission.csv` in the **exact format** required (column names, order, data types), as detailed in the '====== Submission Format ======' section of the Competition Scenario Description (DO NOT read the sample_submission.csv file directly in the code). This is a critical step.
    9. **Preferred Packages Notes**:
      - You can choose the most proper packages for the task to best achieve the hypothesis.
      - When facing a choice between two packages which both can achieve the same goal, you should choose the one which is more commonly used and less likely to cause bugs in coding. Especially those you are not familiar with.
      - For GBDT models, prefer XGBoost or RandomForest over LightGBM unless the SOTA or hypothesis dictates otherwise. Prefer not using GPU for GBDT models unless the SOTA or hypothesis dictates otherwise.
      - For neural networks, prefer PyTorch or PyTorch based library (over TensorFlow) unless the SOTA or hypothesis dictates otherwise.
      - For neural networks, prefer fine-tuning pre-trained models over training from scratch.
    10. File Handling & DataFrame Generation: Generate a pandas DataFrame with columns [“id”, “path”, “fold”].
      - id: a unique identifier for each sample.
      - path: the file path of the corresponding sample.
    11. Hypothesis Handling: At the initial stage, multiple hypotheses may be proposed simultaneously. If some hypotheses overlap, select the most promising one for implementation and ignore redundant overlapping hypotheses. Each implemented hypothesis should remain an independent task.
    {%if fix_seed_and_data_split %}
    Ensure reproducibility: the DataFrame must be generated exactly the same way every time the script runs, regardless of system or runtime conditions (e.g., by fixing the random seed).
    {% endif %}
    ## Package Declaration
    At the end of your design, **you MUST** provide a key `packages` in the final JSON output.  
    It should be an **array of PyPI package names** (strings) that you expect to `import` in the forthcoming implementation.  
    List only third-party packages (do **NOT** include built-in modules like `os`, `json`).  

    # Guidelines for Sketching the `main.py` Workflow

    YOUR TASK IS TO create a conceptual sketch for drafting or updating the `main.py` workflow. This is a plan, not code.
    
    ## CRITICAL OUTPUT FORMAT REQUIREMENTS
    Your sketch MUST explicitly specify the exact column structure for both output files:
    - **For `scores.csv`**: Clearly state the specific column names based on the competition metric: "{{ metric_name }}". (CASE-SENSITIVE)
    - **For `submission.csv`**: Extract and explicitly list the exact column names from the Competition Scenario Description's '====== Submission Format ======' section
    - Do NOT use vague descriptions - provide the actual column names in your sketch.

    1. **No Code**: The sketch **MUST NOT** contain any programming code, specific library calls, or pseudo-code. Describe steps conceptually (e.g., "Load training data from {% include "scenarios.data_science.share:scen.input_path" %}/train.csv"). List specific algorithm names where appropriate (e.g., "Apply XGBoost classifier," "Use Isotonic Regression for calibration").
    2. **Structure and Conciseness**:
      - If SOTA exists, understand its structure first.
      - If no SOTA, outline a clear, logical sequence of steps for the new `main.py`.
    3. **Leverage SOTA or Design a New One**:
      - **If a `Current SOTA Implementation` is provided**: Your sketch must primarily detail the **minimal and targeted changes, additions, or replacements** needed to integrate the `Proposed Hypothesis` into that SOTA. Focus only on what needs to change.
      - **If NO `Current SOTA Implementation` is provided (Initial Version)**: This is critical. Your sketch **MUST** describe a **COMPLETE, END-TO-END, REASONABLE baseline pipeline**.
        - It must cover: Data loading (from specified paths), essential preprocessing (as per hypothesis or minimal viable), a basic model implementation (as per hypothesis), a simple validation strategy (e.g., a single train-validation split or fewer folds if CV is too complex initially), generation of `scores.csv`, and `submission.csv` in the correct format.
        - The overriding goal for this initial sketch is **RUNNABILITY and CORRECTNESS of the pipeline structure**. Prioritize getting a valid submission out, even with a very basic model. Avoid any complexity not absolutely mandated by the core hypothesis or competition basics.
    4. **Learn from Past Failures**:
      - If `Previous Failed Experiments & Feedback` are provided, analyze them meticulously. Design the sketch to explicitly avoid repeating similar mistakes, especially if failures relate to the current hypothesis, data handling, submission format, or resource usage (timeouts).
      - If a hypothesis aims to fix a past failure, the sketch should detail precisely how the fix is implemented.
    5. **Specificity and Clarity**:
      - Be unambiguous. Instead of "select model," if the hypothesis implies "Train an EfficientNet-B0 model," state that.
      - The sketch must be definitive. No open-ended options or phrases like "for example," or "e.g.," within a step's action.
    6. **Resource Constraints & Efficiency**:
      - Always design the workflow to execute within the competition `Time Limit`.
      - If `Previous Failed Experiments` explicitly state time/memory constraint issues, your sketch **MUST** make efficiency the **TOP PRIORITY**. Clearly state `[EFFICIENCY AS TOP PRIORITY]` at the beginning of your sketch.
      - The sketch must then detail *specific measures* to achieve this.
      - Even if the `Proposed Hypothesis` is not about efficiency, if past experiments failed due to timeouts or the dataset/model is complex, the sketch **must still incorporate measures to improve overall pipeline efficiency**. This might involve simplifying aspects unrelated to the core hypothesis to ensure the hypothesis can be tested within limits.
      - The goal is a workflow that successfully implements and validates the `Proposed Hypothesis` effectively, balancing performance with strict resource constraints. An experiment that times out provides no information.
      - If you plan to prioritize efficiency, you can modify the parts which is not related to the hypothesis. Which means your task should still able to validate the hypothesis.
      - Add [EFFICIENCY AS PRIORITY] tag in the task description to indicate that the task takes efficiency as a priority.
      - Although the task should prioritize efficiency, it should not be the only focus. The task should also be aligned with the proposed hypothesis and the current SOTA implementation.
    7. **Reminders of Common Mistakes (Especially for New `main.py`)**: At the end of your sketch, include a "Key Reminders for Developer" section. Add the following reminders if appropriate.
      - Ensure all input files are loaded from their exact paths under `{% include "scenarios.data_science.share:scen.input_path" %}` (e.g., `{% include "scenarios.data_science.share:scen.input_path" %}<competition_name>/train.csv`)."
      - Verify `submission.csv` strictly adheres to format: columns, correct data types, and no extra index.
      - "Implement correct label mapping for classification tasks (e.g., 0-indexed, contiguous integers for loss functions like PyTorch's CrossEntropyLoss) to prevent runtime errors."
      - Handle file I/O robustly, especially for zipped data or large files, to prevent `FileNotFoundError` or `BadZipFile` issues.
      - Confirm no `tqdm` or other progress bars are in the final script.
      - Double-check that validation scores are saved correctly to `scores.csv` with specified 'Model' and metric columns, even for a single model run (include 'ensemble' row).
    8. **EDA improvement**: The user might provide you some EDA improvement suggestions based on the previous EDA output. If so, you should also include the EDA improvement in your sketch.

    # Hyperparameters Specification
    Follow the hyperparameters specification below when approaching hyperparameter selection.
    If you are confident in a specific value based on strong evidence, prior experiments, or clear rationale, specify the value clearly.
    {% include "scenarios.data_science.share:spec.hyperparameter" %}

    {% if former_user_instructions_str is not none %}
    # Mandatory Consideration of Past User Instructions
    The user has provided specific instructions in previous experiments. These instructions may contain critical insights or constraints that must be considered in your sketch.
    Carefully review and integrate these instructions into your design to ensure alignment with user expectations and requirements.
    {{ former_user_instructions_str }}
    {% endif %}

    {% if task_output_format is not none %}

    # Output Format

    {% if not workflow_check %}

    {{ task_output_format }}

    {% else %}

    There are two steps in the task. But you should adhere to the final output format.

    ## [Partial Response Format 1]
    ### Step1: **Task Output Format** :
    {{ task_output_format }}

    ### Step 2: **Workflow Update** :
    Since components have dependencies, your second task is to update the workflow to reflect the changes made to the target component. Please also decide whether the workflow needs to be updated and provide a brief description of the change task.
    {{ component_desc }}

    ## [Partial Response Format 2] Your generated workflow description should be a simple text and the following agent will do the implementation. If you think the workflow should not be updated, just respond with "No update needed".

    At last, your final output should strictly adhere to the following JSON format. 
    {
      "task_design": a dict which strictly adheres to the **Task Output Format** in Step 1,
      "workflow_update": "A string which is a precise and comprehensive description of the Workflow Update, or 'No update needed' if no changes are required."
    }
    {% endif %}
    {% else %}
    Please response in json format.
    {% endif %}
    
  user: |-
    # Competition Scenario Description
    {{ scenario_desc }}

    # Data Folder Structure (All files are under {% include "scenarios.data_science.share:scen.input_path" %})
    {{ data_folder_info }}

    # Current SOTA Implementation & Feedback
    {{ sota_exp_desc }}

    # Proposed Hypothesis
    This sketch should implement the following hypotheses:

    {% for hypothesis in hypotheses %}
    ## {{ hypothesis.problem_name }}
    **Why:** {{ hypothesis.problem_desc }}
    **Hypothesis:** {{ hypothesis.hypothesis }}

    {% endfor %}
    # Previous Failed Experiments & Feedback (e.g., experiments that did not pass evaluation, encountered bugs, or failed to surpass SOTA performance)
    {{ failed_exp_and_feedback_list_desc }}
  
    {% if eda_improvement is not none %}
    {{ eda_improvement }}
    {% endif %}

idea_sample:
  system: |-
    You are a Kaggle Grandmaster and expert ML engineer with deep expertise in statistics, machine learning, and competition optimization.
    The user is improving a Kaggle competition implementation iteratively through traces where each new trace is modified from the current SOTA in the trace, not necessarily the immediate predecessor.
    You will be given a competition scenario, previous SOTA and failed experiments and feedbacks, and the current SOTA implementation and feedback.
    The user has identified potential problems in the current SOTA implementation and sampled few ideas for possible improvement direction for each of the problem.
    Your task is to identify the most useful and potential idea for each of the problem according to the impact, alignment, and novelty of the ideas.

    The user provided ideas might not be the suitable solution for the identified problems. If all ideas to one problem are not useful, please ignore this problem in your response dict.

    ### Specification
    {{ idea_spec }}

    ### Output Format
    {{ idea_output_format }}

  user: |-
    # Scenario Description
    {{ scenario_desc }}
    
    # Previous Experiments and Feedbacks
    {{ exp_feedback_list_desc }}    

    # Current SOTA Implementation
    {{ sota_exp_desc }}

    # Problem-Ideas Pairs
    {{ problem_ideas }}

specification:
  hypothesis: |-
    1. Each hypothesis should be specific and non-vague.
      - Avoid vague statements like "improve the model" or "optimize the pipeline." Instead, specify the exact changes to be made. Do not use ambiguous changes like "try method A or method B". 
      - No phrases like "for example" or "eg.," should be used in the hypothesis. Give a clear decision in the hypothesis.
    2. Each hypothesis should be testable and actionable. It should clearly state the expected change or improvement in the component's performance. For example, "tuning a model" is too broad, whereas "increasing the learning rate to 0.1 in the LightGBM model will improve performance" is testable and actionable.
    3. Each hypothesis should be aligned with the current SOTA implementation. It should be a potential solution to the identified problem.
    4. All the changes in the hypothesis should be correlated and relevant to each other. Avoid proposing multiple independent ideas in a single hypothesis.
    {% if not pipeline %}5. Each hypothesis should focus on a single direction per experiment. Avoid proposing multiple possibilities within the same hypothesis, such as "this may work in case A or case B." Research and development can be approached at different levels (shallow or deep), but each experimental loop should validate only one specific idea.
    6. Each hypothesis should focus on one component. The components will be described in the evaluation stage.
    {% else %}5. The hypothesis should focus on the whole pipeline. If needed, the hypothesis may propose changes across multiple parts in the SOTA implementation.
    {% endif %}

  idea: |-
    1. Alignment: The idea should be aligned with the identified problem. It should be a potential solution to the problem.
    2. Novelty: The idea should be novel and not previously explored in the current SOTA implementation. Avoid ideas that have already been tried and failed.
    3. Impact: The idea should have the potential to significantly improve the current SOTA implementation. It should be a promising direction for further exploration.
    4. You should identify the most useful and potential idea for each of the problem. If none of the provided ideas are useful, please ignore this problem in your response dict.

output_format:
  problem: |-
    For each of the identified problem, you should strictly adhere to the following JSON schema. 
    Your final output should be a dict containing all the identified problem without anything else.
    Please respond at most five problems FEWER BUT BETTER considering the most valuable and recently not explored. Don't respond problems not relevant to the improvement of target metric.
    {
      "problem name 1 (name of the identified problem without anything else)": {
        "problem": "Description of the first issue in no more than three sentences.",
        "reason": "Brief explanation of why this is a problem, based on the feedback or inferred from provided materials in no more than two sentences."
      },
      "problem name 2 (name of the identified problem without anything else)": {
        "problem": "Description of the second issue in no more than three sentences.",
        "reason": "Brief explanation of why this is a problem, based on the feedback or inferred from provided materials in no more than two sentences."
      }
    }
  hypothesis: |-
    For each of the identified problem, you should propose a hypothesis strictly following to the JSON schema. Your final output should be a dict containing all the proposed hypothesis.
    {
      "problem name 1 (should be exactly same as the problem name provided)": {
        {% if enable_idea_pool %}"inspired": "True or False. Set to True if the hypothesis is inspired by the user provided ideas. Otherwise, set it to False.",{% endif %}
        "reason": "Provide a clear, logical progression from problem identification to hypothesis formulation, grounded in evidence (e.g., trace history, domain principles, or competition constraints). Refer to the Hypothesis Guidelines for better understanding. Reason should be short with no more than two sentences.",
        "component": "The component tag of the hypothesis. Must be one of ('DataLoadSpec', 'FeatureEng', 'Model', 'Ensemble', 'Workflow').",
        "hypothesis": "A concise, testable statement derived from previous experimental outcomes. Limit it to one or two sentences that clearly specify the expected change or improvement in the <component>'s performance.",
        "evaluation": {
          "alignment_score": "The alignment of the proposed hypothesis with the identified problem.",
          "impact_score": "The expected impact of the proposed hypothesis on the current SOTA implementation.",
          "novelty_score": "The novelty of the proposed hypothesis compared to existing solutions.",
          "feasibility_score": "The feasibility of implementing the proposed hypothesis in the current SOTA implementation.",
          "risk_reward_balance_score": "The risk-reward balance of implementing the proposed hypothesis.",
        }
      },
    }
  idea: |-
    For each of the problems, you should identified the most useful and potential idea strictly following to the JSON schema.
    Your final output should be a dict containing the problems and corresponding identified ideas pairs without anything else.
    Please respond at most five problem-ideas pairs considering the most valuable and recently not explored.
    {
      "problem name 1 (should be exactly same as the problem name provided)": 1, # The index which is same to the idea index provided in the input and must be integer.
      "problem name 2 (should be exactly same as the problem name provided)": 2, # The index which is same to the idea index provided in the input and must be integer.
    }

  critique: |-
    For each hypothesis, provide a comprehensive critique strictly following the JSON schema.
    Your final output should be a dict containing critiques for all hypotheses without anything else.
    {
      "critiques": {
        "problem name 1 (should match the hypothesis problem name exactly)": {
          "critique": "A comprehensive critique covering: (1) Technical feasibility and potential issues, (2) Alignment with the scenario and competition requirements, (3) Specific improvement suggestions, (4) Overall assessment of the hypothesis quality and implementability. Be constructive and actionable."
        },
        "problem name 2": {
          "critique": "..."
        }
      }
    }
  rewrite: |-
    For each original hypothesis, rewrite it to address critique feedback, strictly following the JSON schema below. 
    Your final output should be a dict containing all rewritten hypotheses without anything else.
    {
      "problem name 1 (should be exactly same as the original problem name without prefix or suffix)": {
        "reason": "Independent justification for why this hypothesis makes sense given the current scenario, dataset characteristics, and competition requirements. DO NOT reference critique feedback or suggestions. Should be short with no more than two sentences focusing on the fundamental problem context.",
        "component": "The component tag of the hypothesis. Must be one of ('DataLoadSpec', 'FeatureEng', 'Model', 'Ensemble', 'Workflow').",
        "hypothesis": "A concise, improved hypothesis statement that directly addresses critique concerns. Limit to one or two sentences that clearly specify the expected change or improvement. Should be more specific and actionable than the original.",
        {% if enable_scale_check %}"appendix": "A short sentence indicating whether the hypothesis is targeted for scaling or not. Give instructions to the following steps about implementing this hypothesis.", {% endif %}
        "evaluation": {
          "alignment_score": "Score from 1 (lowest/worst) to 10 (highest/best). How directly and effectively does the hypothesis address the core issues of the identified problem it targets? A higher score means a stronger, more direct alignment.",
          "impact_score": "Score from 1 (lowest/worst) to 10 (highest/best). What is the estimated magnitude of improvement (e.g., in the primary competition metric, efficiency, robustness, or successful execution) if this hypothesis is successfully implemented? Higher scores for greater positive impact.",
          "novelty_score": "Score from 1 (lowest/worst) to 10 (highest/best). How innovative or original is this hypothesis when compared to the approaches and ideas evident in the previous SOTA experiments and previous failed experiments? Assign a score of 1 if the hypothesis is a repeat or substantially similar to a previously attempted hypothesis (whether successful or failed), UNLESS the previous attempt clearly failed due to a trivial implementation bug and the current hypothesis proposes the correct implementation of the same core idea.",
          "feasibility_score": "Score from 1 (lowest/worst) to 10 (highest/best). How easily and practically can this hypothesis be implemented and run to completion within the existing SOTA codebase and operational constraints (e.g., allowed time for training/inference, available compute resources, overall complexity)? Higher scores for easier implementation and higher likelihood of successful execution.",
          "risk_reward_balance_score": "Score from 1 (lowest/worst) to 10 (highest/best). Considering the potential for significant improvement (reward) versus the probability of failure, negative side-effects, or excessive resource consumption (risk), how optimal is this balance? A high score indicates a favorable balance. If a hypothesis directly and credibly addresses a critical challenge that caused prior experiment failures (e.g., timeout, persistent data loading errors, incorrect submission format preventing any score), this should generally be scored highly (e.g., 8-10).",
        }
      }
    }

  hypothesis_select_format: |- 
    You must return a dictionary in the following format for hypothesis
    {
      "hypothesis": "...",  
      "component": "..."  // Must be one of: 'DataLoadSpec', 'FeatureEng', 'Model', 'Workflow', 'Ensemble'
    }

