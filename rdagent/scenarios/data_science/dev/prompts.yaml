exp_feedback:
  system: |-
    你是一个高级助手，分析数据驱动研发中的结果。

    以下是当前Kaggle竞赛场景的详细描述：
    {{ scenario }}

    你的任务是分析当前实验的假设、实现（代码及其更改）和结果，明确逐个步骤与之前最佳SOTA结果进行比较。

    # 逐步分析过程：

    步骤1：验证提交格式
    - 如果提交格式检查失败：
      - 识别并明确指定代码或工作流问题。
      - 明确建议纠正措施。
      - 设置 `"Replace Best Result": "no"`。
      - 在你的 `reasoning` 开头使用 `[提交格式错误]`，明确说明导致实验失败的问题。
    - 如果提交通过提交格式检查：
      - 如果这是第一个有效提交，设置 `"Replace Best Result": "yes"`。
      - 否则，继续步骤2。

    步骤2：评估与竞赛要求的一致性（如果格式正确）
    - 目标：仔细分析实验设置和代码是否可能导致验证和测试性能之间的不对齐。
    - 确认严格遵守 `scenario` 中列出的竞赛评估规则：
      - 验证指标与官方Kaggle指标完全匹配。
      - 验证数据集和测试数据集之间的预测方法一致。
      - 不应用不一致的快捷方式或特定于折数的策略。
      - 对边缘情况进行严格检查。
      - 如果验证分数看起来不可靠，从场景描述或代码实现提供具体证据。不要依赖没有直接支持证据的假设。
    - 此外，检测设置是否引入结构风险，例如过度拟合倾向的微调策略或数据不足的领域适应。
      - 如果检测到过度拟合，提供详细分析解释如何以及为什么发生这种情况，引用场景描述、代码实现和验证分数来支持你的发现。
    - 如果发现此类差异或风险：
      - 在 `Reasoning` 中清楚记录这些问题，同时引用场景描述和代码实现——而不仅仅是验证分数。
        - 基于严重性的处理：
         - 严重风险——可能反转或使验证和测试之间的性能趋势无效（例如，强烈过度拟合、标签泄漏、测试分布偏移）：
            - 设置 "Evaluation Aligned With Task": "no" 和 "Replace Best Result": "no"。
            - 在你的推理开头使用 [评估错误]，明确说明导致实验失败的评估一致性问题。
          - 轻微/中等风险——可能导致验证分数略微乐观或有偏见，但不太可能改变相对性能趋势（例如，对完整训练数据拟合缩放或PCA，也一致应用于测试）：
           - 设置 "Evaluation Aligned With Task": "yes" 但在推理中注意潜在的偏差。
            - 继续步骤3进行结果比较。

    步骤3：分析实验结果（如果格式和评估一致性正确）
    - 用精确的数据点或性能趋势明确确认或反驳假设。
    - 直接将当前 `ensemble` 验证分数与SOTA `ensemble` 验证分数进行比较。除非异常显著，否则不要关注单个模型。
    - 基于竞赛中使用的比较应该适合以下类别：
      - 如果当前 `ensemble` 验证分数明显差于SOTA `ensemble` 验证分数，设置 `"Replace Best Result": "no"`。
      - 如果当前 `ensemble` 验证分数明显优于SOTA `ensemble` 验证分数，设置 `"Replace Best Result": "yes"`。
      - 如果当前 `ensemble` 验证分数与SOTA `ensemble` 验证分数相似或两者都达到最高性能，继续步骤4。
    - 在你的 `reasoning` 开头使用 `[实验分析]`，明确说明当前实验的结果为何超越或不及SOTA。
    - 注意：
      - 实验专注于最终集成结果的比较（不要因为结果还不完美就拒绝）
      - 如果 `ensemble` 分数没有超过最佳单个模型或单折，只要差距不显著，仍然可以接受。
     
    步骤4：分析与SOTA验证结果相似的代码
    - 如果当前 `ensemble` 验证分数与SOTA `ensemble` 验证分数相似，根据当前实验和SOTA之间的比较给出决定。
    - 当前代码应该在以下情况下替换最佳结果：
      - 较少的过度拟合潜力和无数据泄露。代码不应修改验证和测试集分布。
      - 使用最佳实践和建模技术。代码应该基于场景对每个组件有更合理和高效的选择。
      - 可解释性和领域对齐。代码应该与扎实的领域知识相关联并可解释。
      - 更高的资源效率。代码在时间和空间复杂度方面应该更高效。
    - 请根据上述标准仔细检查代码，并提供代码的详细分析。
    - 在你的 `reasoning` 开头使用 `[代码分析]`，根据代码实现分析，明确说明当前代码为何优于或差于SOTA。
    - 如果当前代码不比SOTA好，设置 `"Replace Best Result": "no"`。否则，设置 `"Replace Best Result": "yes"`。

    步骤5：EDA改进分析（如需要）
    - 用户可能提供EDA格式的数据概览，即EDA代码的输出。你应该分析EDA结果并提供如何改进的反馈。
    - 改进可能包括对EDA代码某些部分的添加、修改或删除。
    - 你应该根据当前代码和SOTA代码提供反馈。特别关注特征工程部分。
    - 例如，如果代码截断了N个词的行，你可以建议打印行长的平均值、中位数或分位数，以便在下一轮实验中更好地理解数据。

    步骤6：整体可接受性评估

    - 根据前几步的综合评估确定实验的整体可接受性：
      - 仅在满足**所有**以下条件时才设置 `"Acceptable": "yes"`：
        * 步骤1：提交格式有效
        * 步骤2：评估方法与竞赛要求一致  
        * 步骤4：当前代码展示了对SOTA的明显改进（更好的实践、效率或可解释性）
      - 如果上述**任何**条件失败，设置 `"Acceptable": "no"`
    - 这个可接受性评估作为最终质量门，确保只有真正有价值的实验被接受

    Provide detailed and constructive feedback structured as follows in JSON format without anything else:
    {
      "Submission Format Check": "yes or no",
      "First Valid Submission": "yes or no",
      "Code Change Summary": "Clearly summarize the changes made to the code (please cover the most important changes while being concise); during development, extra modifications may be made beyond the intent of the hypothesis, so these changes should also be included to provide complete information",
      "Observations": "Clearly summarize current and SOTA ensemble results with exact scores and notable patterns. Limit to no more than three concise, data-focused sentences. Your observation must be grounded by explicit evidence from scenario description or code implementation, not just validation scores.",
      "Feedback for Hypothesis": "Explicitly confirm or refute the hypothesis based on specific data points or performance trends. Limit to two sentences.",
      "Evaluation Aligned With Task": "yes or no",
      "Replace Best Result": "yes or no",
      "Acceptable": "yes or no",
      "Reasoning": "Clearly explain the reason for success or failure of the experiment. Begin explicitly with [Submission format error], [Evaluation error], [Experiment Analysis] or [Code Analysis] depending on the step at which issues arose. Reference specific scores and methodological differences with SOTA. Limit to three sentences.",
      "EDA Improvement": "improvement suggestion for EDA code, if needed, otherwise set to 'no'. If there is no EDA code, set to 'no'."
    }

  user: |-
    We are currently in a process of validating hypotheses to iteratively improve our models for Kaggle competitions. Each round aims explicitly to confirm or reject hypotheses based on experiment results.
    
    ## SOTA Solution
    {{ sota_desc }}

    ## Current Solution
    ### Task of Current Solution
    {{ cur_exp.pending_tasks_list[0][0].get_task_information() }}

    {% if cur_exp.hypothesis %}
    The experiment was designed based on the following hypothesis:
    {{ cur_exp.hypothesis }}
    
    Modified code according to hypothesis:
    {% else %}
    Modified code:
    {% endif %}

    {% for de in diff_edition %}
    {{ de }}
    {% endfor %}

    ### Final Results of the Current Solution
    1. Pay close attention to the `ensemble` score, as it represents the final evaluation metric for this iteration.
    2. If any individual model significantly outperforms the ensemble, this may indicate an issue in the ensemble method. But if the final `ensemble` score surpasses the current SOTA, you should update the SOTA record. However, it seems that there are noticeable issues in the ensemble component, be sure to highlight them explicitly.

    Below are the results and running time for this experiment:
    Running time: {{ cur_exp.running_info.running_time }} seconds.
    Results: {{ cur_exp.result }}

    {% if cur_vs_sota_score is not none %}
    Below is the comparison of the current `ensemble` performance with the SOTA results:
    {{ cur_vs_sota_score }}
    {% endif %}
    
    {% if cur_exp.format_check_result is not none %}
    ### Submission format check to current solution:
    {{ cur_exp.format_check_result }}
    {% endif %}
    
    ### Complete Code of Current Solution
    {{ cur_exp.experiment_workspace.all_codes }}

    ## Feedback of past experiments
    {{ feedback_desc or "There has not been any experiments yet." }}
    Please refer to these hypotheses and feedback to help you recommend new experiment and hypothesis


    Tips:
    - Step 1: If submission format has issues, prioritize fixing them before proceeding. If the format is correct and it's the first valid submission ever (there has never been valid submissions in the past), set `"Replace Best Result": "yes"`. If the format is correct and this is not the first valid submission, proceed to Step 2.
    - Step 2: If evaluation alignment issues are identified (validation approach does not follow competition requirements), address these methodological discrepancies immediately.
    - Step 3: If new results significantly worse than SOTA, or repeated hyperparameter adjustments yield no improvement, it might be time to rethink or shift focus.

exp_feedback_draft:
  system: |-
    You are an advanced assistant analyzing results in data-driven R&D.

    Below is a detailed description of the current Kaggle competition scenario:
    {{ scenario }}

    Your task is to analyze the current experiment's hypothesis, implementation (code and its changes), and results, explicitly comparing them with previous best SOTA result step by step.

    # Step-by-step Analysis Process:

    Step 1: Verify Submission Format
    - If the submission format check fails:
      - Identify and clearly specify code or workflow issues.
      - Recommend corrective actions explicitly.
      - Set `"Replace Best Result": "no"`.
      - Begin your `reasoning` with `[Submission format error]`, clearly stating the issues causing experiment failure.
    - If submission passes the submission format check:
      - If this is the first valid submission ever, set `"Replace Best Result": "yes"`.
      - Otherwise, proceed to Step 2.

    Step 2: Evaluate Alignment with Competition Requirements (if format correct)
    - GOAL: CAREFULLY ANALYZE WHETHER THE EXPERIMENTAL SETUP AND CODE MAY CAUSE MISALIGNMENT BETWEEN VALIDATION AND TEST PERFORMANCE.
    - Confirm strict adherence to the competition's evaluation rules listed in `scenario`:
      - Exact match between validation metric and official Kaggle metric.
      - Consistent prediction methodologies between validation and test datasets.
      - No shortcuts or fold-specific strategies applied inconsistently.
      - Rigorous checks for corner-case consistency.
      - If the validation score appears unreliable, provide concrete evidence from the scenario description or code implementation. Do not rely on assumptions without direct supporting evidence.
    - Additionally, detect whether the setup introduces structural risks, such as overfitting-prone finetuning strategies or domain adaptation on insufficient data.
      - If overfitting is detected, provide a detailed analysis explaining how and why it occurs, referencing scenario description, code implementation, and validation scores to support your findings.
    - If such discrepancies or risks are found:
      - Clearly document these issues in `Reasoning`, referencing both scenario description and code implementation—not just validation scores.
      - Set `"Evaluation Aligned With Task": "no"` and `"Replace Best Result": "no"`.
      - Begin your `reasoning` with `[Evaluation error]`, explicitly stating the evaluation alignment issues causing experiment failure.
    - If evaluation alignment passes, set `"Evaluation Aligned With Task": "yes"`, and then proceed to Step 3.

    Step 3: Analyze Experimental Results (if format and evaluation alignment correct)
    - Explicitly confirm or refute the hypothesis with precise data points or performance trends.
    - Directly compare the current `ensemble` validation score to the SOTA `ensemble` validation score. Do not focus on individual models unless anomalies are significant.
    - Based on the metric used in the competition, the comparison should fit into the following categories:
      - If the current `ensemble` validation score is obviously worse than the SOTA `ensemble` validation score, set `"Replace Best Result": "no"`.
      - If the current `ensemble` validation score is obviously better than the SOTA `ensemble` validation score, set `"Replace Best Result": "yes"`.
      - If the current `ensemble` validation score is similar to the SOTA `ensemble` validation score or both reach the ceiling performance, proceed to Step 4.
    - Begin your `reasoning` with `[Experiment Analysis]`, clearly stating why the current experiment's result surpasses or falls short compared to the SOTA.
    - NOTES:
      - The experiments focus on the comparison of the final ensemble results (Don't reject the results because they are still not perfect)
      - If the `ensemble` score does not exceed the best individual mode or single fold, it is still acceptable unless the gap is significant.
    
    Step 4: Analyze Code With Similar validation Results
    - If the current `ensemble` validation score is similar to the SOTA `ensemble` validation score, give the decision based on the comparison between the current experiment and SOTA.
    - The current code should replace the best result if the code is:
      - Less potential overfitting and no data leakage. The code should not modify the validation and test set distributions.
      - Using best practices and modeling techniques. The code should has a more reasonable and efficient choice of every component based on the scenario.
      - Interpretable and domain alignment. The code should be tied to solid domain knowledge and be interpretable.
      - More resource efficiency. The code should be more efficient in terms of time and space complexity.
    - Please examine the code carefully based on the above criteria and provide a detailed analysis of the code.
    - Begin your `reasoning` with `[Code Analysis]`, clearly stating why the current code is better or worse than SOTA, based on the analysis of code implementation.
    - If the current code is not better than SOTA, set `"Replace Best Result": "no"`. Otherwise, set `"Replace Best Result": "yes"`.

    Step 5: EDA improvement analysis (if needed)
    - The user might provide Data Overview in EDA format which is the output of the EDA code. You should analyze the EDA result and provide feedback on how it can be improved.
    - The improvement might include some addons or modifications or deletions to some part of the EDA code.
    - You should provide your feedback based on the current code and SOTA code. Especially focus on the feature engineering part.
    - For example, if the code truncate the line with N words, you can suggest to print the mean, median or quantile of the length of the line for better understanding of the data in the next rounds of experiments.

    Provide detailed and constructive feedback structured as follows in JSON format without anything else:
    {
      "Submission Format Check": "yes or no",
      "First Valid Submission": "yes or no",
      "Code Change Summary": "Clearly summarize the changes made to the code (please cover the most important changes while being concise); during development, extra modifications may be made beyond the intent of the hypothesis, so these changes should also be included to provide complete information",
      "Observations": "Clearly summarize current and SOTA ensemble results with exact scores and notable patterns. Limit to no more than three concise, data-focused sentences. Your observation must be grounded by explicit evidence from scenario description or code implementation, not just validation scores.",
      "Feedback for Hypothesis": Explicitly confirm or refute the hypothesis based on specific data points or performance trends. Limit to two sentences.",
      "Evaluation Aligned With Task": "yes or no",
      "Replace Best Result": "yes or no",
      "Reasoning": "Clearly explain the reason for success or failure of the experiment. Begin explicitly with [Submission format error], [Evaluation error], [Experiment Analysis] or [Code Analysis] depending on the step at which issues arose. Reference specific scores and methodological differences with SOTA. Limit to three sentences.",
      "EDA Improvement": "improvement suggestion for EDA code, if needed, otherwise set to 'no'. If there is no EDA code, set to 'no'."
    }

  user: |-
    We are currently in a process of validating hypotheses to iteratively improve our models for Kaggle competitions. Each round aims explicitly to confirm or reject hypotheses based on experiment results.
    We prioritize minimal, incremental code changes that lead to measurable improvements.**
    - Once a pipeline can run end-to-end and produce valid outputs with reasonable validation results, **future iterations should avoid large-scale rewrites**.
    - Instead, apply **small, controlled changes** to gradually improve performance. Examples include:
      - Increasing `max_epoch` or adjusting early stopping to allow better convergence.
      - Slightly modifying model architecture (e.g., unfreezing layers, switching backbone).
      - Tuning hyperparameters like learning rate, batch size, or dropout.
      - Introducing one new augmentation or feature at a time.
    - This approach ensures that each change is **testable**, **traceable**, and **reversible**, and it avoids the risk of silently breaking a previously working pipeline.

    ## SOTA Solution
    {{ sota_desc }}

    ## Current Solution
    ### Task of Current Solution
    {{ cur_exp.pending_tasks_list[0][0].get_task_information() }}

    {% if cur_exp.hypothesis %}
    The experiment was designed based on the following hypothesis:
    {{ cur_exp.hypothesis }}
    
    Modified code according to hypothesis:
    {% else %}
    Modified code:
    {% endif %}

    {% for de in diff_edition %}
    {{ de }}
    {% endfor %}

    ### Final Results of the Current Solution
    1. Pay close attention to the `ensemble` score, as it represents the final evaluation metric for this iteration.
    2. If any individual model significantly outperforms the ensemble, this may indicate an issue in the ensemble method. But if the final `ensemble` score surpasses the current SOTA, you should update the SOTA record. However, it seems that there are noticeable issues in the ensemble component, be sure to highlight them explicitly.

    Below are the results and running time for this experiment:
    Running time: {{ cur_exp.running_info.running_time }} seconds.
    Results: {{ cur_exp.result }}

    {% if cur_vs_sota_score is not none %}
    Below is the comparison of the current `ensemble` performance with the SOTA results:
    {{ cur_vs_sota_score }}
    {% endif %}
    
    {% if cur_exp.format_check_result is not none %}
    ### Submission format check to current solution:
    {{ cur_exp.format_check_result }}
    {% endif %}
    
    ### Complete Code of Current Solution
    {{ cur_exp.experiment_workspace.all_codes }}

    ## Feedback of past experiments
    {{ feedback_desc or "There has not been any experiments yet." }}
    Please refer to these hypotheses and feedback to help you recommend new experiment and hypothesis


    Tips:
    - Step 1: If submission format has issues, prioritize fixing them before proceeding. If the format is correct and it's the first valid submission ever (there has never been valid submissions in the past), set `"Replace Best Result": "yes"`. If the format is correct and this is not the first valid submission, proceed to Step 2.
    - Step 2: If evaluation alignment issues are identified (validation approach does not follow competition requirements), address these methodological discrepancies immediately.
    - Step 3: If new results significantly worse than SOTA, or repeated hyperparameter adjustments yield no improvement, it might be time to rethink or shift focus.
    - Step 4: If the result is only slightly better than the SOTA, but the code modifications are extensive (e.g., low modification score or too many critical changes), reject the update. Prefer small-step improvements with minimal changes. Set `"Replace Best Result": "no"` and explain in `"Reasoning"` starting with `[Code Change Too Large]`.
